defaults:
  - agents
  - _self_

# ============================================================================
# How to adjust training hyperparameters
#
# Edit this file to control most training behavior:
# - rollout: agent grouping, validation, filtering, and reward normalization
# - ppo: PPO algorithm knobs (epochs, minibatch, gamma, lambda, entropy, clips)
# - training: optimizer, grad accumulation, eval cadence, and checkpointing
#   NOTE: set max_steps / eval_every_n_steps to -1 to use script defaults,
#         set a positive integer here to override explicitly.
# - rollout_runtime: decoding limits and sampling settings for rollout
# - model.repo_id: base model to download and run
#
# The script composes this config with defaults (e.g., agents.yaml) automatically
# and prints the merged configuration once at startup for transparency.
# ============================================================================

# ------ Rollout Configuration ------
rollout:
  agent_group_num: [8]
  agent_group_size: [16] # number of agents = agent_group_num * agent_group_size
  validation_agent_group_num: [128]
  validation_agent_group_size: [1]
  training: [simpleSokobanAgent]
  validation: [simpleSokobanAgent]
  validation_seed: 123
  truncation: left  # truncate from left (oldest tokens) to keep recent context
  use_turn_scores: False  # for GAE computation
  rollout_filter_ratio: 0.25  # filter ratio for rollout selection
  rollout_filter_type: std  # std or std_rev for filtering criteria
  reward_normalization:
    grouping: "state" # state / batch / inductive
    method: "identity" # asym_clip / identity / mean_std
  # Lightweight CPU parallelism for prompt building and env stepping
  # 0 or unset -> auto = min(32, os.cpu_count())
  num_prompt_threads: 0
  num_env_threads: 0
  # Show tqdm progress bars for concurrent steps
  show_tqdm: False

# ------ Cluster configuration (RLCluster) ------
cluster:
  mesh:
    shape: [2, 2]
    axes: ["fsdp", "tp"]
  # role_to_mesh can be a mapping per role in future. For now "same" means
  # use the single mesh for all roles.
  role_to_mesh: same
  rollout_engine: vanilla  # vanilla or vllm
  offload_to_cpu: false
  training_config:
    # Scheduling and evaluation
    mini_batch_size: 32
    training_micro_batch_size: 4
    eval_every_n_steps: -1  # -1 to let script derive
    max_steps: -1  # -1 to let script derive
    checkpoint:
      save_interval_steps: 500
      max_to_keep: 1
  rollout_config:
    train:
      max_prompt_length: 2048
      total_generation_steps: 100
      temperature: 1.0
      top_p: 1.0
      top_k: null
    eval:
      max_prompt_length: 2048
      total_generation_steps: 100
      temperature: 1.0
      top_p: 1.0
      top_k: null

# ------ Trainer configuration ------
trainer:
  ppo:
    num_ppo_epochs: 1
    gamma: 1.0
    gae_lambda: 1.0
    beta: 0.001
    epsilon: 0.2
    clip_range_value: 0.5
    entropy_coef: 0.001
    epsilon_low: 0.2
    epsilon_high: 0.28
    epsilon_c: 3.0
    kl_penalty_method: k1
  optim:
    gradient_accumulation_steps: 8
    actor_lr: 1.0e-6
    critic_lr: 1.0e-5
    b1: 0.9
    b2: 0.999
    weight_decay: 0.01
    max_grad_norm: 1.0
    type: constant

# ------ Model artifacts ------
model:
  repo_id: Qwen/Qwen2.5-0.5B-Instruct

