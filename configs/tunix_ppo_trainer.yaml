# Tunix PPO/GRPO trainer config (derived from tunix/examples/grpo_demo.ipynb)
# Organized similarly to ppo_trainer.yaml, but targeting Tunix RL APIs.

# ====== Data ======
data:
  # Training and test data directories
  train_data_dir: ./data/train
  test_data_dir: ./data/test

  # Fraction of the dataset used for training (1.0 = all)
  train_fraction: 1.0

  # Per-step batch size (used in the demo for dataset batching)
  batch_size: 1


# ====== LoRA (matches demo constants RANK/ALPHA) ======
lora:
  rank: 64
  alpha: 64.0


# ====== Mesh / Sharding ======
mesh:
  # Passed to jax.make_mesh(*MESH). The demo uses [(1, 4), ("fsdp", "tp")].
  dims:
    - [1, 4]
    - [fsdp, tp]


# ====== Rollout / Sampling during RL (maps to base_rollout.RolloutConfig) ======
rollout:
  # Matches MAX_PROMPT_LENGTH / TOTAL_GENERATION_STEPS from the demo
  max_prompt_length: 256
  max_tokens_to_generate: 768

  # Tunable sampling knobs from the demo
  temperature: 0.9
  top_p: 1.0
  top_k: 50

  # KV cache size used by vanilla sampler
  kv_cache_size: 1280  # MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256


# ====== GRPO hyperparameters (from the demo) ======
grpo:
  num_generations: 2       # G in the paper
  num_iterations: 1        # Î¼ in the paper
  beta: 0.08               # KL penalty coefficient
  epsilon: 0.2             # PPO-style clip epsilon


# ====== Training loop and optimization ======
training:
  # Batches/epochs used to derive total steps (see demo comments)
  num_batches: 3738
  num_test_batches: 100
  eval_every_n_steps: 10
  num_epochs: 1

  # Computed in the demo as: int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)
  # With defaults above: 3738 * 1 * 1.0 * 1 = 3738
  max_steps: 3738

  # AdamW optimizer (from demo block)
  optimizer:
    type: adamw
    learning_rate: 3.0e-6
    b1: 0.9
    b2: 0.99
    weight_decay: 0.1
    # In the demo WARMUP_STEPS = 0.1 * MAX_STEPS; keep explicit for clarity
    warmup_steps: 374
    max_grad_norm: 0.1

  # Metrics logging (tensorboard in demo)
  metrics_logging_options:
    log_dir: /content/tmp/tensorboard/grpo
    flush_every_n_steps: 20

  # Checkpointing
  checkpoint:
    intermediate_ckpt_dir: /content/intermediate_ckpt/
    ckpt_dir: /content/ckpts/
    save_interval_steps: 500
    max_to_keep: 4


# ====== Cluster / RL Orchestration (maps to rl_cluster.ClusterConfig) ======
cluster:
  # Map model roles to mesh. The demo colocates all on the same mesh.
  role_to_mesh:
    actor: mesh
    reference: mesh
    rollout: mesh

  # Use the Tunix vanilla rollout engine (Sampler-based)
  rollout_engine: vanilla
  offload_to_cpu: false



